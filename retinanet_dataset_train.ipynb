{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, \"/home/vision\")\n",
    "\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-christopher",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import xml.dom.minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDataset(Dataset):\n",
    "    def __init__(self, image_dir=\"./data\", annotations_dir=\"./ann\",transform=None):\n",
    "        self.files_name = os.listdir(image_dir)\n",
    "        self.image_dir = image_dir\n",
    "        self.annotaions_dir = annotations_dir\n",
    "        self.transforms = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files_name)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        file_name, _ = os.path.splitext(self.files_name[idx])\n",
    "        img_path = os.path.join(self.image_dir, file_name + \".png\")\n",
    "        xml_path = os.path.join(self.annotaions_dir, file_name + \".xml\")\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        ann = self.read_annotaions(xml_path)\n",
    "        lbl = [1 for _ in range(len(ann))]\n",
    "        \n",
    "        target = {\"boxes\": ann, \"labels\": lbl}\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        return img, target\n",
    "        \n",
    "    def read_annotaions(self, xml_path):\n",
    "        res = []\n",
    "        \n",
    "        dom = xml.dom.minidom.parse(xml_path)\n",
    "        root = dom.documentElement\n",
    "        objects=dom.getElementsByTagName(\"object\")\n",
    "        for obj in objects:\n",
    "            bndbox = obj.getElementsByTagName('bndbox')[0]\n",
    "            xmin = bndbox.getElementsByTagName('xmin')[0]\n",
    "            ymin = bndbox.getElementsByTagName('ymin')[0]\n",
    "            xmax = bndbox.getElementsByTagName('xmax')[0]\n",
    "            ymax = bndbox.getElementsByTagName('ymax')[0]\n",
    "            xmin_data=xmin.childNodes[0].data\n",
    "            ymin_data=ymin.childNodes[0].data\n",
    "            xmax_data=xmax.childNodes[0].data\n",
    "            ymax_data=ymax.childNodes[0].data\n",
    "            res.append([int(xmin_data),\n",
    "                        int(ymin_data),\n",
    "                        int(xmax_data),\n",
    "                        int(ymax_data)])\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        imgs = [item[0] for item in batch]\n",
    "        trgts = [item[1] for item in batch]\n",
    "        \n",
    "        return [imgs, trgts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        for key, value in target.items():\n",
    "            target[key] = torch.as_tensor(np.array(value), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target\n",
    "    \n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "    \n",
    "transform = Compose([ToTensor(),\n",
    "                     Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_notf = BirdDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = ds_notf[1]\n",
    "for box in item[1][\"boxes\"]:\n",
    "    cv2.rectangle(item[0], (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 2)\n",
    "plt.imshow(item[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-merchant",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BirdDataset(transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset=ds, collate_fn=ds.collate_fn, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.retinanet_resnet50_fpn(num_classes=2,\n",
    "                                                            pretrained=False,\n",
    "                                                            pretrained_backbone=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "epochs = 10\n",
    "\n",
    "epoch_loss = []\n",
    "for epoch in range(1, epochs):\n",
    "    for i_batch, batch in enumerate(dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        losses = model(*batch)\n",
    "\n",
    "        loss = losses[\"classification\"] + losses[\"bbox_regression\"]\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(float(loss))\n",
    "        \n",
    "        print('Epoch: {} | batch: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(\n",
    "            epoch,\n",
    "            i_batch+1,\n",
    "            float(losses[\"classification\"].detach().numpy()),\n",
    "            float(losses[\"bbox_regression\"].detach().numpy()),\n",
    "            float(loss.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./chpt.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-access",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./chpt.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predicted = model([ds[2][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = torchvision.ops.nms(predicted[0][\"boxes\"], predicted[0][\"scores\"], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = ds_notf[2]\n",
    "img = item[0]\n",
    "oboxes = item[1][\"boxes\"]\n",
    "\n",
    "keep = keep.numpy()\n",
    "boxes = list(np.floor(predicted[0][\"boxes\"].detach().numpy()[keep]))\n",
    "scores = list(predicted[0][\"scores\"].detach().numpy()[keep])\n",
    "\n",
    "print(len(oboxes))\n",
    "for box, score in zip(boxes, scores):\n",
    "    if score > 0.2:\n",
    "        cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 0, 255), 2)\n",
    "        \n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-remove",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
